{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ezBIDS CLI Demo: UK Biobank Example DICOMs\n\nThis notebook demonstrates converting DICOM data to BIDS format using `ezbids-cli`.\n\nWe'll showcase:\n- **Multi-modality conversion**: T1w, FLAIR, T2*w (SWI), resting-state fMRI, and DWI\n- **Automatic detection**: How ezbids identifies datatypes, suffixes, and entities\n- **Two-stage workflow**: Analyze data, review, then apply conversion\n- **Configuration files**: Reusable settings for consistent conversions\n- **Validation**: BIDS compliance checking\n\nData source: [UK Biobank Example DICOMs](https://biobank.ndph.ox.ac.uk/ukb/label.cgi?id=507)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install dependencies and configure working directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install ezbids-cli (uncomment one option)\n# !pip install ezbids-cli  # From PyPI\n!pip install -e .. --quiet  # From local source (development)\n\n# Install visualization dependencies\n!pip install nibabel matplotlib --quiet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport shutil\nimport zipfile\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\n\nimport matplotlib.pyplot as plt\nimport nibabel as nib\nimport numpy as np\n\n# Working directories\nWORK_DIR = Path(\"demo_data\")\nDICOM_DIR = WORK_DIR / \"dicoms\"\nBIDS_DIR = WORK_DIR / \"bids_output\"\nANALYSIS_DIR = WORK_DIR / \"analysis\"\n\nfor d in [WORK_DIR, DICOM_DIR, ANALYSIS_DIR]:\n    d.mkdir(exist_ok=True)\n\nprint(f\"Working directory: {WORK_DIR.absolute()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Download UK Biobank Example DICOMs\n\nUK Biobank provides publicly accessible example DICOM datasets. We'll download five modalities to demonstrate multi-modal conversion:\n\n| Modality | BIDS Type | Description |\n|----------|-----------|-------------|\n| T1 | anat/T1w | Structural MPRAGE |\n| T2 FLAIR | anat/FLAIR | Fluid-attenuated inversion recovery |\n| SWI | anat/T2starw | Susceptibility-weighted imaging |\n| Resting fMRI | func/bold | Resting-state functional MRI |\n| DWI | dwi/dwi | Diffusion-weighted imaging |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK Biobank example DICOM URLs\n",
    "UKB_EXAMPLES = {\n",
    "    \"t1\": \"https://biobank.ndph.ox.ac.uk/ukb/ukb/examples/eg_brain_t1.zip\",\n",
    "    \"t2flair\": \"https://biobank.ndph.ox.ac.uk/ukb/ukb/examples/eg_brain_t2flair.zip\",\n",
    "    \"rest_fmri\": \"https://biobank.ndph.ox.ac.uk/ukb/ukb/examples/eg_brain_rest.zip\",\n",
    "    \"dwi\": \"https://biobank.ndph.ox.ac.uk/ukb/ukb/examples/eg_brain_mbdif.zip\",\n",
    "    \"swi\": \"https://biobank.ndph.ox.ac.uk/ukb/ukb/examples/eg_brain_suswt.zip\",\n",
    "}\n",
    "\n",
    "def download_and_extract(name: str, url: str, target_dir: Path) -> Path:\n",
    "    \"\"\"Download and extract a zip file.\"\"\"\n",
    "    zip_path = target_dir / f\"{name}.zip\"\n",
    "    extract_dir = target_dir / name\n",
    "    \n",
    "    if extract_dir.exists():\n",
    "        print(f\"  {name}: already exists, skipping\")\n",
    "        return extract_dir\n",
    "    \n",
    "    print(f\"  {name}: downloading...\", end=\" \", flush=True)\n",
    "    urlretrieve(url, zip_path)\n",
    "    \n",
    "    print(\"extracting...\", end=\" \", flush=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    zip_path.unlink()\n",
    "    \n",
    "    file_count = sum(1 for f in extract_dir.rglob(\"*\") if f.is_file())\n",
    "    print(f\"done ({file_count} files)\")\n",
    "    return extract_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download all five modalities\ndatasets_to_download = [\"t1\", \"t2flair\", \"swi\", \"rest_fmri\", \"dwi\"]\n\nprint(\"Downloading UK Biobank example DICOMs...\\n\")\nfor name in datasets_to_download:\n    download_and_extract(name, UKB_EXAMPLES[name], DICOM_DIR)\n\nprint(\"\\nDownload complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what we downloaded\n",
    "print(\"Downloaded DICOM directories:\")\n",
    "print(\"=\" * 40)\n",
    "for d in sorted(DICOM_DIR.iterdir()):\n",
    "    if d.is_dir():\n",
    "        file_count = sum(1 for f in d.rglob(\"*\") if f.is_file())\n",
    "        print(f\"  {d.name}/ ({file_count} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-Command Conversion\n",
    "\n",
    "The simplest way to use ezbids-cli: a single command that handles everything.\n",
    "\n",
    "```bash\n",
    "ezbids convert <input_dir> --output-dir <output_dir>\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Discover all DICOM files\n",
    "2. Convert to NIfTI using dcm2niix\n",
    "3. Identify datatypes and suffixes (T1w, FLAIR, bold, dwi, etc.)\n",
    "4. Extract BIDS entities (task, direction, run, etc.)\n",
    "5. Organize into BIDS directory structure\n",
    "6. Validate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean previous output if it exists\n",
    "if BIDS_DIR.exists():\n",
    "    shutil.rmtree(BIDS_DIR)\n",
    "\n",
    "# Run the conversion\n",
    "!ezbids convert {DICOM_DIR} --output-dir {BIDS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the resulting BIDS structure\n",
    "def show_tree(path: Path, prefix: str = \"\", max_depth: int = 4, current_depth: int = 0):\n",
    "    \"\"\"Display directory tree.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted(path.iterdir())\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        connector = \"\\u2514\\u2500\\u2500 \" if is_last else \"\\u251c\\u2500\\u2500 \"\n",
    "        print(f\"{prefix}{connector}{item.name}\")\n",
    "        \n",
    "        if item.is_dir():\n",
    "            extension = \"    \" if is_last else \"\\u2502   \"\n",
    "            show_tree(item, prefix + extension, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"BIDS Output Structure\")\n",
    "print(\"=\" * 50)\n",
    "if BIDS_DIR.exists():\n",
    "    show_tree(BIDS_DIR)\n",
    "else:\n",
    "    print(\"No output yet - run the conversion cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualize the Converted Data\n\nLet's view the converted NIfTI files:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all NIfTI files in the BIDS output\nnifti_files = sorted(BIDS_DIR.rglob(\"*.nii.gz\"))\n\nprint(\"Converted NIfTI files:\")\nprint(\"-\" * 60)\nfor f in nifti_files:\n    # Show path relative to BIDS dir\n    rel_path = f.relative_to(BIDS_DIR)\n    # Get image dimensions\n    img = nib.load(f)\n    shape = img.shape\n    print(f\"  {rel_path}  {shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_slices(nifti_path, title=None, vol_idx=0):\n    \"\"\"Plot orthogonal slices from a NIfTI file.\"\"\"\n    img = nib.load(nifti_path)\n    data = img.get_fdata()\n    \n    # Handle 4D data by selecting a volume\n    if data.ndim == 4:\n        data = data[..., vol_idx]\n        title_suffix = f\" (vol {vol_idx})\"\n    else:\n        title_suffix = \"\"\n    \n    # Get middle slices\n    mid_x, mid_y, mid_z = [s // 2 for s in data.shape[:3]]\n    \n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    axes[0].imshow(np.rot90(data[mid_x, :, :]), cmap='gray')\n    axes[0].set_title('Sagittal')\n    axes[0].axis('off')\n    \n    axes[1].imshow(np.rot90(data[:, mid_y, :]), cmap='gray')\n    axes[1].set_title('Coronal')\n    axes[1].axis('off')\n    \n    axes[2].imshow(np.rot90(data[:, :, mid_z]), cmap='gray')\n    axes[2].set_title('Axial')\n    axes[2].axis('off')\n    \n    if title:\n        fig.suptitle(f\"{title}{title_suffix}\", fontsize=12)\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View the T1w anatomical scan\nt1_files = [f for f in nifti_files if \"T1w\" in f.name]\nif t1_files:\n    plot_slices(t1_files[0], title=t1_files[0].name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# View the FLAIR scan\nflair_files = [f for f in nifti_files if \"FLAIR\" in f.name]\nif flair_files:\n    plot_slices(flair_files[0], title=flair_files[0].name)"
  },
  {
   "cell_type": "code",
   "source": "# View the SWI scan (T2*-weighted)\nswi_files = [f for f in nifti_files if \"T2starw\" in f.name]\nif swi_files:\n    plot_slices(swi_files[0], title=swi_files[0].name)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# View the resting-state fMRI (first volume of 4D)\nbold_files = [f for f in nifti_files if \"bold\" in f.name]\nif bold_files:\n    plot_slices(bold_files[0], title=bold_files[0].name, vol_idx=0)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# View the DWI scan (first volume of 4D)\ndwi_files = [f for f in nifti_files if f.name.endswith(\"_dwi.nii.gz\")]\nif dwi_files:\n    plot_slices(dwi_files[0], title=dwi_files[0].name, vol_idx=0)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding What Was Detected\n",
    "\n",
    "Let's use the programmatic API to understand exactly what ezbids detected and how it made its decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezbids_cli.core.analyzer import Analyzer\n",
    "\n",
    "# Run analysis (without conversion) to inspect what was detected\n",
    "analyzer = Analyzer(DICOM_DIR, ANALYSIS_DIR)\n",
    "analysis = analyzer.analyze()\n",
    "\n",
    "print(f\"Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Subjects found: {len(analysis.get('subjects', []))}\")\n",
    "print(f\"Acquisitions found: {len(analysis.get('objects', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show detailed detection results for each acquisition\nprint(\"Detected Acquisitions\")\nprint(\"=\" * 70)\n\nfor i, obj in enumerate(analysis.get(\"objects\", []), 1):\n    datatype = obj.get(\"datatype\", \"unknown\")\n    suffix = obj.get(\"suffix\", \"unknown\")\n    bids_type = f\"{datatype}/{suffix}\" if datatype and suffix else \"unidentified\"\n    entities = obj.get(\"entities\", {})\n    series_desc = obj.get(\"SeriesDescription\", \"N/A\")\n    \n    print(f\"\\n[{i}] {bids_type}\")\n    print(f\"    Series: {series_desc}\")\n    \n    # Show extracted entities\n    entity_items = [(k, v) for k, v in entities.items() if k not in [\"subject\", \"session\"] and v]\n    if entity_items:\n        entity_str = \", \".join(f\"{k}={v}\" for k, v in entity_items)\n        print(f\"    Entities: {entity_str}\")\n    \n    # Show any messages or warnings\n    if obj.get(\"message\"):\n        print(f\"    Note: {obj['message']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the JSON sidecar for one acquisition\n",
    "# This shows the metadata extracted from DICOM headers\n",
    "\n",
    "# Find a JSON sidecar file\n",
    "json_files = sorted(BIDS_DIR.rglob(\"*.json\"))\n",
    "# Filter to acquisition sidecars (not dataset_description.json, etc.)\n",
    "sidecar_files = [f for f in json_files if f.parent.name in [\"anat\", \"func\", \"dwi\", \"fmap\", \"perf\"]]\n",
    "\n",
    "if sidecar_files:\n",
    "    example_sidecar = sidecar_files[0]\n",
    "    print(f\"Example JSON sidecar: {example_sidecar.name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    with open(example_sidecar) as f:\n",
    "        sidecar_data = json.load(f)\n",
    "    \n",
    "    # Show key fields\n",
    "    key_fields = [\n",
    "        \"Modality\", \"MagneticFieldStrength\", \"Manufacturer\", \"ManufacturersModelName\",\n",
    "        \"RepetitionTime\", \"EchoTime\", \"FlipAngle\", \"SliceThickness\",\n",
    "        \"PhaseEncodingDirection\", \"EffectiveEchoSpacing\",\n",
    "    ]\n",
    "    \n",
    "    for field in key_fields:\n",
    "        if field in sidecar_data:\n",
    "            print(f\"  {field}: {sidecar_data[field]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Two-Stage Workflow\n",
    "\n",
    "For more control, you can separate analysis from conversion:\n",
    "\n",
    "1. **Analyze**: Detect and classify acquisitions, save to JSON\n",
    "2. **Review**: Inspect/modify the analysis (manually or via TUI)\n",
    "3. **Apply**: Convert using the reviewed analysis\n",
    "\n",
    "This is useful when you want to:\n",
    "- Review what will be converted before committing\n",
    "- Manually correct misidentified acquisitions\n",
    "- Exclude certain scans from conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean previous analysis output to avoid duplicates\nif ANALYSIS_DIR.exists():\n    shutil.rmtree(ANALYSIS_DIR)\nANALYSIS_DIR.mkdir(exist_ok=True)\n\n# Stage 1: Analyze and save results\n!ezbids analyze {DICOM_DIR} --output-dir {ANALYSIS_DIR}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the analysis output files\n",
    "print(\"Analysis output files:\")\n",
    "print(\"-\" * 40)\n",
    "for f in sorted(ANALYSIS_DIR.iterdir()):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the analysis JSON\n",
    "analysis_file = ANALYSIS_DIR / \"ezBIDS_core.json\"\n",
    "\n",
    "if analysis_file.exists():\n",
    "    with open(analysis_file) as f:\n",
    "        saved_analysis = json.load(f)\n",
    "    \n",
    "    print(\"Analysis file structure:\")\n",
    "    print(\"-\" * 40)\n",
    "    for key in saved_analysis.keys():\n",
    "        value = saved_analysis[key]\n",
    "        if isinstance(value, list):\n",
    "            print(f\"  {key}: [{len(value)} items]\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  {key}: {{...}}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could modify the analysis here programmatically\n",
    "# For example, to exclude an acquisition or change its type:\n",
    "#\n",
    "# saved_analysis[\"objects\"][0][\"_exclude\"] = True\n",
    "# saved_analysis[\"objects\"][1][\"_datatype\"] = \"anat\"\n",
    "# saved_analysis[\"objects\"][1][\"_suffix\"] = \"T2w\"\n",
    "#\n",
    "# with open(analysis_file, \"w\") as f:\n",
    "#     json.dump(saved_analysis, f, indent=2)\n",
    "\n",
    "print(\"Analysis ready for review.\")\n",
    "print(\"\\nTo use the interactive TUI reviewer, run:\")\n",
    "print(f\"  ezbids review {analysis_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Apply the analysis to create BIDS output\n",
    "BIDS_DIR_STAGED = WORK_DIR / \"bids_staged\"\n",
    "if BIDS_DIR_STAGED.exists():\n",
    "    shutil.rmtree(BIDS_DIR_STAGED)\n",
    "\n",
    "!ezbids apply {analysis_file} {BIDS_DIR_STAGED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the staged output matches\n",
    "print(\"Staged BIDS Output\")\n",
    "print(\"=\" * 50)\n",
    "if BIDS_DIR_STAGED.exists():\n",
    "    show_tree(BIDS_DIR_STAGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration-Based Conversion\n",
    "\n",
    "For reproducible conversions across multiple datasets, you can use a YAML configuration file.\n",
    "\n",
    "This is useful for:\n",
    "- Applying the same settings to multiple subjects/sessions\n",
    "- Sharing conversion settings with collaborators\n",
    "- Documenting exactly how data was converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a config file from existing analysis\n",
    "!ezbids init-config {DICOM_DIR} --output {WORK_DIR}/my_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the generated config\n",
    "config_file = WORK_DIR / \"my_config.yaml\"\n",
    "if config_file.exists():\n",
    "    print(\"Generated configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(config_file.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a custom config\n",
    "custom_config = \"\"\"\n",
    "version: \"1.0\"\n",
    "\n",
    "dataset:\n",
    "  name: \"UK Biobank Demo\"\n",
    "  bids_version: \"1.9.0\"\n",
    "  authors:\n",
    "    - \"ezBIDS CLI Demo\"\n",
    "\n",
    "# Series matching rules (applied in order)\n",
    "series:\n",
    "  # T1-weighted structural\n",
    "  - match:\n",
    "      series_description: \".*T1.*MPRAGE.*\"\n",
    "    datatype: anat\n",
    "    suffix: T1w\n",
    "  \n",
    "  # FLAIR\n",
    "  - match:\n",
    "      series_description: \".*FLAIR.*\"\n",
    "    datatype: anat\n",
    "    suffix: FLAIR\n",
    "  \n",
    "  # Resting-state fMRI\n",
    "  - match:\n",
    "      series_description: \".*rest.*fMRI.*\"\n",
    "    datatype: func\n",
    "    suffix: bold\n",
    "    entities:\n",
    "      task: rest\n",
    "  \n",
    "  # DWI\n",
    "  - match:\n",
    "      series_description: \".*dMRI.*\"\n",
    "    datatype: dwi\n",
    "    suffix: dwi\n",
    "\n",
    "output:\n",
    "  link_mode: hardlink  # hardlink, symlink, or copy\n",
    "  validate: true\n",
    "\"\"\"\n",
    "\n",
    "custom_config_path = WORK_DIR / \"custom_config.yaml\"\n",
    "custom_config_path.write_text(custom_config)\n",
    "print(\"Custom config written to:\", custom_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert using the custom config\n",
    "BIDS_DIR_CONFIG = WORK_DIR / \"bids_from_config\"\n",
    "if BIDS_DIR_CONFIG.exists():\n",
    "    shutil.rmtree(BIDS_DIR_CONFIG)\n",
    "\n",
    "!ezbids convert {DICOM_DIR} --config {custom_config_path} --output-dir {BIDS_DIR_CONFIG}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation\n",
    "\n",
    "ezBIDS CLI includes the official `bids-validator` Python package for checking BIDS compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ezbids_cli.validation.validator import validate_dataset, print_validation_result\n\n# The BIDS dataset is in the 'dataset' subdirectory\nbids_dataset_dir = BIDS_DIR / \"dataset\"\n\nprint(\"Validating BIDS Dataset\")\nprint(\"=\" * 50)\nprint(f\"Path: {bids_dataset_dir}\\n\")\n\nresult = validate_dataset(bids_dataset_dir)\nprint_validation_result(result, verbose=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also validate from the command line:\n",
    "# !ezbids validate {BIDS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect Dataset Metadata\n",
    "\n",
    "Let's look at the dataset-level files that were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# dataset_description.json\nbids_dataset_dir = BIDS_DIR / \"dataset\"\ndesc_file = bids_dataset_dir / \"dataset_description.json\"\nif desc_file.exists():\n    print(\"dataset_description.json\")\n    print(\"=\" * 40)\n    with open(desc_file) as f:\n        print(json.dumps(json.load(f), indent=2))\nelse:\n    print(f\"File not found: {desc_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# participants.tsv\nparticipants_file = bids_dataset_dir / \"participants.tsv\"\nif participants_file.exists():\n    print(\"participants.tsv\")\n    print(\"=\" * 40)\n    print(participants_file.read_text())\nelse:\n    print(f\"File not found: {participants_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "Remove downloaded data and outputs when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all demo data\n",
    "# shutil.rmtree(WORK_DIR)\n",
    "# print(\"Cleaned up demo data\")\n",
    "\n",
    "# Show disk usage\n",
    "import subprocess\n",
    "result = subprocess.run([\"du\", \"-sh\", str(WORK_DIR)], capture_output=True, text=True)\n",
    "print(f\"Demo data size: {result.stdout.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}